<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Research on @adlrocha</title>
    <link>/tags/research/</link>
    <description>Recent content in Research on @adlrocha</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 18 Feb 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="/tags/research/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Running LLMs and ML in Wasm</title>
      <link>/blog/2024-02-18-wasm-llm/</link>
      <pubDate>Sun, 18 Feb 2024 00:00:00 +0000</pubDate>
      <guid>/blog/2024-02-18-wasm-llm/</guid>
      <description>&lt;h1 id=&#34;running-llms-and-ml-in-wasm&#34;&gt;Running LLMs and ML in Wasm&lt;/h1&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;Searching new runtimes for AI&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;p&gt;I came up with an (obvious) idea the other day that led to me to the following question: &lt;em&gt;&amp;ldquo;would it be possible to run LLM inference from Wasm?&amp;rdquo;&lt;/em&gt;. Being able to compile ML models into Wasm would allow us to run them in a heterogeneous set of runtimes and devices, including mobile or the browser. However, would running these models in Wasm offer access to the low-level computational resources of the device required for an efficient execution?&lt;/p&gt;</description>
    </item>
    <item>
      <title>Late arrival to the fuss of LLMs</title>
      <link>/blog/2024-01-18-intro-llms/</link>
      <pubDate>Sun, 21 Jan 2024 00:00:00 +0000</pubDate>
      <guid>/blog/2024-01-18-intro-llms/</guid>
      <description>&lt;h1 id=&#34;late-arrival-to-the-fuss-of-llms&#34;&gt;Late Arrival to the Fuss of LLMs&lt;/h1&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;From zero to zero-point-one in a few resources&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;p&gt;After spending some time reading about the &lt;a href=&#34;./2023-12-07-state-ai.md&#34;&gt;state of AI&lt;/a&gt; at a high-level, it was time to dive into the details. For obvious reasons, I decided LLMs were a good first-stop for my AI enlightenment. &lt;strong&gt;What are LLMs and how they work (because apparently the why is still a burning open question)?&lt;/strong&gt;.&lt;/p&gt;&#xA;&lt;p&gt;Rivers of digital ink have been spilled lately with gentle introductions and deep illustrations of how LLMs and their underlying transformer architectures work. The good thing about this? There is a lot of information available to learn about them. The bad thing? Filtering the best resources to get you to a good understanding with the lower overhead may be time consuming. Consequently, I decided that instead of writing yet another introductory post about transformers and LLMs, it may be more useful to just share the curated list of resources that have helped me the most on this humble quest. I hope you enjoy it (and if you find any good resource worth including to this list, feel free to send it my way and I&amp;rsquo;ll edit this post).&lt;/p&gt;</description>
    </item>
    <item>
      <title>Making sense of the current state of AI</title>
      <link>/blog/2024-01-10-state-ai/</link>
      <pubDate>Wed, 10 Jan 2024 00:00:00 +0000</pubDate>
      <guid>/blog/2024-01-10-state-ai/</guid>
      <description>&lt;h1 id=&#34;adlrocha---making-sense-of-the-current-state-of-ai&#34;&gt;@adlrocha - Making sense of the current state of AI&lt;/h1&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;Or rather, my limited view as an outsider.&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;p&gt;I&amp;rsquo;ve been trying to stay up to date (at least at a high level) with all the new developments in the field of AI, but with my full-time job it has been quite a challenge. This can be even more of a challenge when your full-time job is not closely related with the field of AI. My only sources of information these days for all AI-related topics have been Twitter and Hacker News. To make matters worse, my ML background is quite limited and dates back to my college years, so I only understand just half of what I read.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Network Coding in P2P Networks</title>
      <link>/blog/2020-08-network-coding/</link>
      <pubDate>Tue, 13 Oct 2020 18:16:08 +0100</pubDate>
      <guid>/blog/2020-08-network-coding/</guid>
      <description>&lt;h1 id=&#34;adlrocha---network-coding-in-p2p-networks&#34;&gt;&lt;a href=&#34;https://adlrocha.substack.com/people/3137214-alfonso-de-la-rocha&#34;&gt;@adlrocha - Network Coding in P2P Networks&lt;/a&gt;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;linear-combinations-on-the-fly&#34;&gt;Linear combinations on-the-fly!&lt;/h3&gt;&#xA;&lt;p&gt;I am not going to lie, writing my Sunday publication is getting harder and harder as I go deeper into my research on file-sharing in P2P networks. I spend almost all of my waking hours working on it, and by the end of the day I don&amp;rsquo;t have any more brain power left to start writing one of these pieces. Fortunately, &lt;strong&gt;I am working on such an exciting field that I can always choose one of the things I&amp;rsquo;ve been exploring throughout the week and just start writing about it.&lt;/strong&gt; And this has been the case for this publication, where I will share my excitement about network coding schemes in P2P networks.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Traversing the NAT</title>
      <link>/blog/2020-10-nat/</link>
      <pubDate>Tue, 13 Oct 2020 18:16:08 +0100</pubDate>
      <guid>/blog/2020-10-nat/</guid>
      <description>&lt;h1 id=&#34;adlrocha---traversing-the-nat&#34;&gt;&lt;a href=&#34;https://adlrocha.substack.com/people/3137214-alfonso-de-la-rocha&#34;&gt;@adlrocha - Traversing the NAT&lt;/a&gt;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;a-quest-for-direct-global-connectivity-between-devices&#34;&gt;A quest for direct global connectivity between devices.&lt;/h3&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://cdn.substack.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F60977c6b-a665-41bc-b7f8-c9332f5a3909_1000x562.jpeg&#34;&gt;&lt;img src=&#34;https://cdn.substack.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F60977c6b-a665-41bc-b7f8-c9332f5a3909_1000x562.jpeg&#34; alt=&#34;aerial photography of lake&#34;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;These past few weeks I&amp;rsquo;ve been pretty obsessed with the future of the Internet, how it may look like, and the technological path towards it. All this started when I jumped into writing &lt;a href=&#34;https://adlrocha.substack.com/p/adlrocha-my-vision-for-a-new-internet&#34;&gt;my personal vision for a new Internet&lt;/a&gt;. A few weeks later I was invited to participate in a discussion panel about the future of the Internet and web3 in the European Blockchain Convention, and as preparation for the panel &lt;a href=&#34;https://adlrocha.substack.com/p/adlrocha-what-the-next-generation&#34;&gt;I decided to interview a bunch of colleagues in the field to learn their thoughts on the matter&lt;/a&gt;. I concluded this series with last week&amp;rsquo;s discussion on &lt;a href=&#34;https://adlrocha.substack.com/p/adlrocha-what-if-we-had-local-first&#34;&gt;local-first software and the technologies to enable it.&lt;/a&gt;&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
