<!DOCTYPE html>
<html lang="en-us">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title> Remember `to(&#39;cpu&#39;)` in Pytorch to release GPU memory | @adlrocha</title>
  <meta name="description" content="Before joining Protocol Labs as a Research Engineer, I worked as a blockchain expert at Telefónica R&amp;D, where I was responsible for the design and development of core technology based on blockchains, distributed systems, and advanced cryptography. My involvement in research and development began at Universidad Politécnica de Madrid, where I worked on topics related to energy efficiency in data centers. My R&amp;D experience also includes research into the compression efficiency of video coding standards at Ericsson Research and projects related to securing interdomain routing protocols at KTH Royal Institute of Technology in Stockholm. I am also an avid reader and basketball lover.">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="robots" content="all,follow">
  <meta name="googlebot" content="index,follow,snippet,archive">
  <meta property="og:title" content="Remember `to(&#39;cpu&#39;)` in Pytorch to release GPU memory" />
<meta property="og:description" content="Remember to(&#39;cpu&#39;) in Pytorch to release GPU memory When I saw that Microsoft had released phi-2, a 2.7B parameters LLM, I thought: &ldquo;this is the perfect excuse to get my hands dirty with LLMs&rdquo;. The model was small enough to test it directly inside Google Colab, as it would fit the 15GiB memory GPUs provided in the free plan.
So without further ado, I opened Google Colab, pip installed HF&rsquo;s transformers library, and wrote the following code snippet to test the model:" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/til/2024-01-11-pytorch-collab-to-cpu/" /><meta property="article:section" content="til" />
<meta property="article:published_time" content="2024-01-20T00:00:00+00:00" />
<meta property="article:modified_time" content="2024-01-20T00:00:00+00:00" />


  <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Remember `to(&#39;cpu&#39;)` in Pytorch to release GPU memory"/>
<meta name="twitter:description" content="Remember to(&#39;cpu&#39;) in Pytorch to release GPU memory When I saw that Microsoft had released phi-2, a 2.7B parameters LLM, I thought: &ldquo;this is the perfect excuse to get my hands dirty with LLMs&rdquo;. The model was small enough to test it directly inside Google Colab, as it would fit the 15GiB memory GPUs provided in the free plan.
So without further ado, I opened Google Colab, pip installed HF&rsquo;s transformers library, and wrote the following code snippet to test the model:"/>

  
  
    
  
  
  <link rel="stylesheet" href="/css/style-dark.css">
  
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
  

  
<link rel="icon" type="image/png" href="/images/favicon.ico" />

  
  
</head>


<body class="max-width mx-auto px3 ltr">
  <div class="content index py4">

    <header id="header">
  <a href="/">
  
    <div id="logo" style="background-image: url(/images/logo.png)"></div>
  
  <div id="title">
    <h1>@adlrocha</h1>
  </div>
  </a>
  <div id="nav">
    <ul>
      <li class="icon">
        <a href="#"><i class="fas fa-bars fa-2x"></i></a>
      </li>
      
        <li><a href="/">Home</a></li>
      
        <li><a href="/blog">All posts</a></li>
      
        <li><a href="/til">TIL</a></li>
      
        <li><a href="https://adlrocha.substack.com/archive">Newsletter</a></li>
      
        <li><a href="/tags">Tags</a></li>
      
        <li><a href="/learning">Learning</a></li>
      
        <li><a href="/patrons">Patrons</a></li>
      
    </ul>
  </div>
</header>



    
<article class="post" itemscope itemtype="http://schema.org/BlogPosting">
  
  <div class="content" itemprop="articleBody">
    
    <p>
      <i>Any comments, contributions, or feedback? Ping me!</i>
    </p>
    <p>
      <span>
        <a href="https://twitter.com/adlrocha?ref_src=twsrc%5Etfw" class="twitter-follow-button"
          data-show-count="false">Follow @adlrocha</a>
        <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
      </span>
      <span>
        <a href="https://twitter.com/intent/tweet?screen_name=adlrocha&ref_src=twsrc%5Etfw"
          class="twitter-mention-button" data-show-count="false">Tweet to @adlrocha</a>
        <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
      </span>
    </p>
    <h1 id="remember-tocpu-in-pytorch-to-release-gpu-memory">Remember <code>to('cpu')</code> in Pytorch to release GPU memory</h1>
<p>When I saw that Microsoft had released <a href="https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/">phi-2</a>, a 2.7B parameters LLM, I thought: <em>&ldquo;this is the perfect excuse to get my hands dirty with LLMs&rdquo;</em>. The model was small enough to test it directly inside Google Colab, as it would fit the 15GiB memory GPUs provided in the free plan.</p>
<p>So without further ado, I opened Google Colab, <code>pip install</code>ed HF&rsquo;s <code>transformers</code> library, and wrote the following code snippet to test the model:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> torch
<span style="color:#f92672">from</span> transformers <span style="color:#f92672">import</span> AutoModelForCausalLM, AutoTokenizer

torch<span style="color:#f92672">.</span>set_default_device(<span style="color:#e6db74">&#34;cuda&#34;</span>)

<span style="color:#75715e"># Phi 2</span>
<span style="color:#75715e"># https://huggingface.co/microsoft/phi-2</span>
model <span style="color:#f92672">=</span> AutoModelForCausalLM<span style="color:#f92672">.</span>from_pretrained(<span style="color:#e6db74">&#34;microsoft/phi-2&#34;</span>, torch_dtype<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;auto&#34;</span>, trust_remote_code<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
tokenizer <span style="color:#f92672">=</span> AutoTokenizer<span style="color:#f92672">.</span>from_pretrained(<span style="color:#e6db74">&#34;microsoft/phi-2&#34;</span>, trust_remote_code<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">phi2_input</span>(text: str):
  inputs <span style="color:#f92672">=</span> tokenizer(text, return_tensors<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;pt&#34;</span>, return_attention_mask<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
  outputs <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>generate(<span style="color:#f92672">**</span>inputs, max_length<span style="color:#f92672">=</span><span style="color:#ae81ff">300</span>)
  text <span style="color:#f92672">=</span> tokenizer<span style="color:#f92672">.</span>batch_decode(outputs)[<span style="color:#ae81ff">0</span>]
  <span style="color:#66d9ef">return</span> text
</code></pre></div><p>The first call to the model worked like a charm:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">test <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;&#39;&#39;
</span><span style="color:#e6db74">Instruct: Would you be able to help me with code written in Rust or Go? Please only answer to my question and tag as &lt;end&gt; when you have finished answering my specific question.
</span><span style="color:#e6db74">Output:
</span><span style="color:#e6db74">&#39;&#39;&#39;</span>
print(phi2_input(test))
</code></pre></div><p>But if I called the code from above again, <strong>it was failing with the following <code>OutOfMemoryError</code> from CUDA</strong>:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-console" data-lang="console">OutOfMemoryError: CUDA out of memory. Tried to allocate 38.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 15.06 MiB is free. Process 4265 has 14.73 GiB memory in use. Of the allocated memory 14.47 GiB is allocated by PyTorch
</code></pre></div><p>I had no idea why, my GPU memory wasn&rsquo;t being released between calls, and I had to restart the runtime in order to get a fresh GPU with all its memory if I wanted to prompt the model again. What could be happening?</p>
<p>After reading some docs, inspecting the GPU memory, and a bit of wandering around, I realized that <code>model.generate</code> was storing <strong>the output tensors of the model generation in GPU memory</strong>, immediately filling the precious memory of my free Google Colab GPU.</p>
<p>The fix was quite simple, I just needed to <strong>send the output tensors back to CPU</strong> when the generation is done (<a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor.to">relevant docs</a>). With this, I was able to prompt the model as much as I wanted without filling up the memory of the GPU.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-diff" data-lang="diff">def phi2_input(text: str):
  inputs = tokenizer(text, return_tensors=&#34;pt&#34;, return_attention_mask=False)
<span style="color:#f92672">- outputs = model.generate(**inputs, max_length=300)
</span><span style="color:#f92672"></span><span style="color:#a6e22e">+ outputs = model.generate(**inputs, max_length=300).to(&#39;cpu&#39;)
</span><span style="color:#a6e22e"></span>  text = tokenizer.batch_decode(outputs)[0]
  return text
</code></pre></div><p>The morale of the story? When using a framework that abstracts you from the low-level details of a technology, is still really important to have a good understanding of what is happening under the hood. In this case, this was a &ldquo;noob&rdquo; mistake from someone that is taking its first baby steps with Pytorch.</p>

    <p>
      <i>Any comments, contributions, or feedback? Ping me!</i>
    </p>
    <p>
      <span>
        <a href="https://twitter.com/adlrocha?ref_src=twsrc%5Etfw" class="twitter-follow-button"
          data-show-count="false">Follow @adlrocha</a>
        <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
      </span>
      <span>
        <a href="https://twitter.com/intent/tweet?screen_name=adlrocha&ref_src=twsrc%5Etfw"
          class="twitter-mention-button" data-show-count="false">Tweet to @adlrocha</a>
        <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
      </span>
    </p>
    
  </div>
</article>


    <footer id="footer">
  <div class="footer-left">
    Copyright  &copy; 2024  @adlrocha 
  </div>
  <div class="footer-right">
    <nav>
      <ul>
         
        <li><a href="/">Home</a></li>
         
        <li><a href="/blog">All posts</a></li>
         
        <li><a href="/til">TIL</a></li>
         
        <li><a href="https://adlrocha.substack.com/archive">Newsletter</a></li>
         
        <li><a href="/tags">Tags</a></li>
         
        <li><a href="/learning">Learning</a></li>
         
        <li><a href="/patrons">Patrons</a></li>
        
      </ul>
    </nav>
  </div>
</footer>


  </div>
</body>

<link rel="stylesheet" href=/lib/font-awesome/css/all.min.css>
<script src=/lib/jquery/jquery.min.js></script>
<script src=/js/main.js></script>
<script data-goatcounter="https://adlrocha.goatcounter.com/count" async src="//gc.zgo.at/count.js"></script>;

</html>