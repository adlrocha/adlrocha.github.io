---
title: "The State of LLMs"
date: "2023-12-07"
draft: false
tags: [AI, research]
---

# The State of the LLM space
> Or rather, my limited view from the outside.

I've been trying to stay up to date (at least at a high level) with all the new developments in the field of AI, but with a full-time job this is quite a challenge. Even more when your full-time job is not closely related with AI. My only sources of information and new developments these days have been Twitter and Hacker News, and my ML background is quite limited and from my college times, so you can imagine that I understand just half of what I read.

That being said, I realized that a good practice to start getting up to speed with AI is to periodically try to write about the things that I've learnt that week. This will help me build a mental model of what the hell is happening with AI these days, and I may learn a thing or two about this field. "_Does this mean that we are back to weekly publications?"_ Hopefully. I really miss my weekly writing habit, and this is as good an excuse as any other.

![Kid learning about AI and looking at its bright future](../images/kid-ai.jpeg)
*Kid learning about AI and looking at its bright future - SDXL*

## A personal high-level overview
In these past few months that I've been paying a bit more of attention to the AI space, these are a few of the things that I've realized:
- __LLMs and foundational models are becoming a commodity__: Training an LLM from scratch is really expensive, and only a limited amount of companies with big pockets and access to a lot of hardware will be able to do so. Fortunately, there are already several foundational models out there. Developers, researches, and new startups have already done amazing things building upon existing open-source foundational models. These models can already have unprecedented impact, and become the core of really innovative new use cases.
<TODO>Research different open-source foundational models coming from Llama and open source. Maybe an image?</TODO>
- __Building narrow agents by fine-tuning foundational models is the next frontier__: While small teams and developers may not be able to train their own foundational models, they may still build really innovative models to solve specific problems. Companies like OpenAI or Google may be racing towards AGI, but I feel there's still a lot of value on solving niche day-to-day issues through AI and LLMs. For this, we don't need AGI or really strong foundational models, fine-tuned version of existing open-source foundational models may be just what that problem needs. By building really good task-specific models we may be building small agents that in the future are able to collaborate towards more complex tasks _(maybe this is the actual path towards AGI?)_. There already a good examples of companies being built on top of this task-specific agents.
<TODO>Fixie.ai and task specific agents.</TODO>
- __Use cases are in exploration mode__: I don't know about you, but while the future use cases and impact of AI as a technology is way clearer than that of block technology (obviously). I still feel we still are in a use case exploration phase for LLMs and the new amazing models we are seeing everyday. Generative models are already creating breathtaking images and outstanding glimpses of real intelligence and creativity. We've found a new superpower, but I am not really sure if we have figured out the best way to use it. ChatGPT et. al are amazing co-pilots for our day-to-day but they aren't still as good as a good Google search or a well-thought and well-explained Stack Overflow post.
- __Production deployment of LLM-based apps will become an issue__:
- __We should worry about alignment__: And I don't know about you, but I am really worried about alignment and all the things that could go wrong with AI. As mentioned above, we found a new superpower of the likes of electricity, fire, or the Internet. We know this could be huge, but we still don't know how to use it, and all the detrimental impact it may have.
<TODO>Some references about alignment.</TODO>

## What's next for me?
<TODO>Read papers, keep up to date with AI, first few pet projects</TODO>

## Any feedback?
I am not an expert on AI or ML, and I am just starting to peek this vast world, so any feedback or suggestions on how to improve my understanding along the way are more than welcome. If everything goes fine, see you next week :)

NOTES: 
- AIs still need humans to create high-quality content: https://twitter.com/Delachica/status/1736320394368311539/photo/1
