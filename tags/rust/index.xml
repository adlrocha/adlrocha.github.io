<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>rust on @adlrocha</title>
    <link>/tags/rust/</link>
    <description>Recent content in rust on @adlrocha</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 18 Feb 2024 00:00:00 +0000</lastBuildDate><atom:link href="/tags/rust/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Running LLMs and ML in Wasm</title>
      <link>/blog/2024-02-18-wasm-llm/</link>
      <pubDate>Sun, 18 Feb 2024 00:00:00 +0000</pubDate>
      
      <guid>/blog/2024-02-18-wasm-llm/</guid>
      <description>Running LLMs and ML in Wasm  Searching new runtimes for AI
 I came up with an (obvious) idea the other day that led to me to the following question: &amp;ldquo;would it be possible to run LLM inference from Wasm?&amp;quot;. Being able to compile ML models into Wasm would allow us to run them in a heterogeneous set of runtimes and devices, including mobile or the browser. However, would running these models in Wasm offer access to the low-level computational resources of the device required for an efficient execution?</description>
    </item>
    
    <item>
      <title>When should one use Rust&#39;s `Pin&lt;&gt;`?</title>
      <link>/til/2023-08-29-rust-pin/</link>
      <pubDate>Tue, 29 Aug 2023 00:00:00 +0000</pubDate>
      
      <guid>/til/2023-08-29-rust-pin/</guid>
      <description>When should one use Rust&amp;rsquo;s Pin&amp;lt;&amp;gt;? The other day I was reviewing some code from the Fendermint project that I am currently contributing to, and I had to ask the author of the PR directly, &amp;ldquo;man! you have to teach me when I am supposed to use Pin&amp;lt;&amp;gt;. I see you are using it all over the place in this API, but I don&amp;rsquo;t know why&amp;rdquo;. His immediate answer was a bit surprising although kind of expected.</description>
    </item>
    
  </channel>
</rss>
