<!DOCTYPE html>
<html lang="en-us">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title> Late arrival to the fuss of LLMs | @adlrocha</title>
  <meta name="description" content="Co-founder and CTO of Finisterra Labs, where we&#39;re organising the world&#39;s structured data. Before founding Finisterra Labs, I worked at Protocol Labs as a Research Engineer, contributing to open-source projects like libp2p, IPFS, and Filecoin. I also worked as a blockchain expert at TelefÃ³nica R&amp;D, where I was responsible for the design and development of core technology based on blockchains, distributed systems, and advanced cryptography. My involvement in research and development began at Universidad PolitÃ©cnica de Madrid, where I worked on topics related to energy efficiency in data centers. My R&amp;D experience also includes research into the compression efficiency of video coding standards at Ericsson Research and projects related to securing interdomain routing protocols at KTH Royal Institute of Technology in Stockholm. I am also an avid reader and basketball lover.">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="robots" content="all,follow">
  <meta name="googlebot" content="index,follow,snippet,archive">
  <meta property="og:title" content="Late arrival to the fuss of LLMs" />
<meta property="og:description" content="Late Arrival to the Fuss of LLMs  From zero to zero-point-one in a few resources
 After spending some time reading about the state of AI at a high-level, it was time to dive into the details. For obvious reasons, I decided LLMs were a good first-stop for my AI enlightenment. What are LLMs and how they work (because apparently the why is still a burning open question)?.
Rivers of digital ink have been spilled lately with gentle introductions and deep illustrations of how LLMs and their underlying transformer architectures work." />
<meta property="og:type" content="article" />
<meta property="og:url" content="/blog/2024-01-18-intro-llms/" /><meta property="article:section" content="blog" />
<meta property="article:published_time" content="2024-01-21T00:00:00+00:00" />
<meta property="article:modified_time" content="2024-01-21T00:00:00+00:00" />


  <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Late arrival to the fuss of LLMs"/>
<meta name="twitter:description" content="Late Arrival to the Fuss of LLMs  From zero to zero-point-one in a few resources
 After spending some time reading about the state of AI at a high-level, it was time to dive into the details. For obvious reasons, I decided LLMs were a good first-stop for my AI enlightenment. What are LLMs and how they work (because apparently the why is still a burning open question)?.
Rivers of digital ink have been spilled lately with gentle introductions and deep illustrations of how LLMs and their underlying transformer architectures work."/>

  
  
    
  
  
  <link rel="stylesheet" href="/css/style-dark.css">
  
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
  

  
<link rel="icon" type="image/png" href="/images/favicon.ico" />

  
  
</head>


<body class="max-width mx-auto px3 ltr">
  <div class="content index py4">

    <header id="header">
  <a href="/">
  
    <div id="logo" style="background-image: url(/images/logo.png)"></div>
  
  <div id="title">
    <h1>@adlrocha</h1>
  </div>
  </a>
  <div id="nav">
    <ul>
      <li class="icon">
        <a href="#"><i class="fas fa-bars fa-2x"></i></a>
      </li>
      
        <li><a href="/">Home</a></li>
      
        <li><a href="/blog">All posts</a></li>
      
        <li><a href="/blunders">Blunders</a></li>
      
        <li><a href="/til">TIL</a></li>
      
        <li><a href="https://adlrocha.substack.com/archive">Newsletter</a></li>
      
        <li><a href="/tags">Tags</a></li>
      
        <li><a href="/learning">Learning</a></li>
      
    </ul>
  </div>
</header>



    
<article class="post" itemscope itemtype="http://schema.org/BlogPosting">
  
  <div class="content" itemprop="articleBody">
    
    <p>
      <i>Any comments, contributions, or feedback? Ping me!</i>
    </p>
    <p>
      <span>
        <a href="https://twitter.com/adlrocha?ref_src=twsrc%5Etfw" class="twitter-follow-button"
          data-show-count="false">Follow @adlrocha</a>
        <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
      </span>
      <span>
        <a href="https://twitter.com/intent/tweet?screen_name=adlrocha&ref_src=twsrc%5Etfw"
          class="twitter-mention-button" data-show-count="false">Tweet to @adlrocha</a>
        <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
      </span>
    </p>
    <h1 id="late-arrival-to-the-fuss-of-llms">Late Arrival to the Fuss of LLMs</h1>
<blockquote>
<p>From zero to zero-point-one in a few resources</p>
</blockquote>
<p>After spending some time reading about the <a href="./2023-12-07-state-ai.md">state of AI</a> at a high-level, it was time to dive into the details. For obvious reasons, I decided LLMs were a good first-stop for my AI enlightenment. <strong>What are LLMs and how they work (because apparently the why is still a burning open question)?</strong>.</p>
<p>Rivers of digital ink have been spilled lately with gentle introductions and deep illustrations of how LLMs and their underlying transformer architectures work. The good thing about this? There is a lot of information available to learn about them. The bad thing? Filtering the best resources to get you to a good understanding with the lower overhead may be time consuming. Consequently, I decided that instead of writing yet another introductory post about transformers and LLMs, it may be more useful to just share the curated list of resources that have helped me the most on this humble quest. I hope you enjoy it (and if you find any good resource worth including to this list, feel free to send it my way and I&rsquo;ll edit this post).</p>
<p><img src="../images/llm-arrive-late.png" alt="A person arriving late to a party where there is a lot going on">
<em>A person arriving late to a party where there is a lot going on - SDXL</em></p>
<h2 id="llms">LLMs</h2>
<p>Large Language Models (LLMs) are this amazing family of NLP (Natural Language Processing) models that have taken the Internet by storm. The core idea is simple, it is a type of sequence-to-sequence deep learning model based on an architecture called the transformer that is <strong>trained to predict the most probable next sequence of words, given some sequence as an input</strong>. Trained over a large enough dataset (like a dataset with all the content in the Internet), leads to the intelligent-like behaviors that we are seeing in systems like OpenAI&rsquo;s, Mixtral or Anthropic&rsquo;s models.</p>
<p>On my journey to gain a better understanding of LLMs, the best introductory resources I&rsquo;ve come across are:</p>
<ul>
<li>
<p>Andrej Karpathy&rsquo;s amazing <a href="https://www.youtube.com/watch?v=zjkBMFhNj_g">&ldquo;1hr Intro to Large Language Models&rdquo;</a> with <strong>an overview of how LLMs work, their current state of development, and interesting ideas</strong> for future work and open problems (like LLM security, tree of thoughts, self-improvement, prompt injection, or the idea of LLM OS). If you have a bit more of time, also worth watching Karpathy&rsquo;s <a href="https://www.youtube.com/watch?v=kCc8FmEb1nY">Let&rsquo;s build GPT: from scratch, in code, spelled out</a>, with a 2h walkthrough of how to write a transformer from scratch, and train it to speak like Shakespeare.</p>
</li>
<li>
<p>From one renowned deep learning expert to another, Jeremy Howard&rsquo;s <a href="https://www.youtube.com/watch?v=jkrNMKz9pWU">&ldquo;A Hacker&rsquo;s Guide to Language Models&rdquo;</a> is the best way to <strong>get your hands dirty programmatically</strong> interacting with ChatGPT, or running an open-source LLMs like Llama. Actually, I used this resource as a base to start tinkering with LLMs myself and run Microsoft&rsquo;s new <a href="https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/">phi-2</a> in a Google Colab notebook.</p>
</li>
<li>
<p>Finally, I would recommend Will Thompson&rsquo;s <a href="https://willthompson.name/what-we-know-about-llms-primer">&ldquo;What We Know About LLMs (Primer)&quot;</a>. It was written in July 2023 but it has pointers to a lot of good resources around buzzword in the world of LLMs like LoRa, InstructGPT, or RLHF. I will write more about these in the future, but this article really helped me understand some of the techniques used to fine-tuning open-source LLMs that I was reading about (and didn&rsquo;t understand). Some of these techniques are the ones that enabled Llama to be run in a Mac without requiring an expensive GPU and loads of memory, or why there are already open-source models that outperform ChatGPT 3.5.</p>
</li>
</ul>
<p>I feel that the resources from above were all that I needed to get a good grasp of LLMs at a high-level, but let me share a few more that I found quite interesting and (in one case thought provoking).</p>
<ul>
<li><a href="https://www.youtube.com/watch?v=Ma4clS-IdhA">Training and deploying open-source LLMs</a>: Good high-level overview of the state of open-source models from end 2023.</li>
<li><a href="https://ai.meta.com/llama/get-started/">Getting started with Llama</a> has an overview of Llama2 and how to start playing with it</li>
<li><a href="https://www.youtube.com/watch?v=JhCl-GeT4jw">Large Language Models and the End of Programming</a> from Matt Welsh&rsquo;s Fixie.ai. The thing that I liked about this talk is how closely related it is with the concept of software 2.0, and using LLMs and agents as CPUs to orchestrate larger systems.</li>
</ul>
<h2 id="the-transformer">The Transformer</h2>
<p>After going through the previous section, it may be obvious that <em>&ldquo;attention is all you need&rdquo;</em>, and that one of the key responsible for the operation of LLMs is the transformer architecture. If I wanted to get a good intuition of how LLMs work, I felt like I first had to understand deeply how transformers worked (and I had literally no idea). In this regard, these were the resources that helped me the most and that I highly recommend.</p>
<ul>
<li><a href="https://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a> visually presents their operation while sharing useful links about core concepts like <a href="https://machinelearningmastery.com/what-are-word-embeddings/">the context and attention of a seq2seq model</a> and <a href="https://machinelearningmastery.com/what-are-word-embeddings/">word embeddings</a>.</li>
<li><a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html">The annotated transformer</a> walks you through the <strong>original transformer paper with code and examples</strong>, and <a href="https://kazemnejad.com/blog/transformer_architecture_positional_encoding/">Transformer Architecture: The Positional Encoding</a> is the best explanation I could found to what of the parts of the transformer architecture that I understood the least after I read the previous articles.</li>
<li>But I have to say, <strong>the key resource that really helped me to (finally) understand what transformers are all about</strong> is Theia Vogel&rsquo;s <a href="https://vgel.me/posts/handmade-transformer/">&ldquo;I made a transformer by hand&rdquo;</a> and her sequel <a href="https://vgel.me/posts/faster-inference/">&ldquo;How to make LLMs go fast&rdquo;</a>.</li>
<li>In line with the above, I also went through <a href="https://blog.briankitano.com/llama-from-scratch/">Llama from scratch</a> to get a better sense of the implementation of more complex transformer architectures.</li>
</ul>
<p>Finally, for those looking to go the extra mile, <a href="https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/">The Transformer Family Version 2.0</a>, <a href="https://magazine.sebastianraschka.com/p/understanding-and-coding-self-attention">Understanding and Coding Self-Attention, Multi-Head Attention, Cross-Attention, and Causal-Attention in LLMs</a>, and this post about quantization <a href="https://timdettmers.com/2022/08/17/llm-int8-and-emergent-features/">LLM.int8() Emergent Features</a> dive into more complex topics, but they really helped me consolidate the basics.</p>
<h2 id="learning-path">Learning path</h2>
<p>And this is just the start, throughout the festive season I&rsquo;ve been crafting a personal AI learning roadmap to start getting up to speed on this field. I want it to be as hands-on as possible. I also want to share my progress publicly so others as illiterate as me in the world of ML can benefit from it. Ready to dive into the world of AI? Let the coding fun begin! ðŸŽ‰</p>

    <p>
      <i>Any comments, contributions, or feedback? Ping me!</i>
    </p>
    <p>
      <span>
        <a href="https://twitter.com/adlrocha?ref_src=twsrc%5Etfw" class="twitter-follow-button"
          data-show-count="false">Follow @adlrocha</a>
        <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
      </span>
      <span>
        <a href="https://twitter.com/intent/tweet?screen_name=adlrocha&ref_src=twsrc%5Etfw"
          class="twitter-mention-button" data-show-count="false">Tweet to @adlrocha</a>
        <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
      </span>
    </p>
    
  </div>
</article>


    <footer id="footer">
  <div class="footer-left">
    Copyright  &copy; 2024  @adlrocha 
  </div>
  <div class="footer-right">
    <nav>
      <ul>
         
        <li><a href="/">Home</a></li>
         
        <li><a href="/blog">All posts</a></li>
         
        <li><a href="/blunders">Blunders</a></li>
         
        <li><a href="/til">TIL</a></li>
         
        <li><a href="https://adlrocha.substack.com/archive">Newsletter</a></li>
         
        <li><a href="/tags">Tags</a></li>
         
        <li><a href="/learning">Learning</a></li>
        
      </ul>
    </nav>
  </div>
</footer>


  </div>
</body>

<link rel="stylesheet" href=/lib/font-awesome/css/all.min.css>
<script src=/lib/jquery/jquery.min.js></script>
<script src=/js/main.js></script>
<script data-goatcounter="https://adlrocha.goatcounter.com/count" async src="//gc.zgo.at/count.js"></script>;

</html>