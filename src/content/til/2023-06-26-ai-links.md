---
title: "June 2023 AI-related Reading List"
date: "2023-06-26"
draft: false
tags: [AI, reading]
---

# June 2023 AI-related Reading List
I just found myself with a few dozens of tabs opened with AI-related resources that I wanted to read. I feel that having them in _RAM_ (i.e. opened in my browser) was starting overwhelm me, as I felt I could never find the time to read them (leading to a really uncomfortable feeling while I had to make progress in my day-to-day job). Hence, I decided to periodically move from _RAM_ to a _"persistent storage"_ my reading lists so that I could pick them up once I have some time for them (and in the process share them with others that my be interested).

> The check-box at the beginning of each item signals if I have read it and process it yet or not.

### Learning Resources
- [ ] [Transformer Architecture: The positional encoding](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/)
- [ ] [Hugging Face's (HF) Transformers Getting Started](https://huggingface.co/docs/transformers/index)
- [ ] [HF's Diffusers Resources](https://huggingface.co/docs/diffusers/index)
- [ ] [What is a vector database?](https://www.pinecone.io/learn/vector-database/)
- [ ] [Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA](https://huggingface.co/blog/4bit-transformers-bitsandbytes)
- [ ] [A Gentle Introduction to 8-bit Matrix Multiplication for transformers at scale using transformers, accelerate and bitsandbytes](https://huggingface.co/blog/hf-bitsandbytes-integration)
- [LLM Powered Autonomous Agents](https://lilianweng.github.io/posts/2023-06-23-agent/)
- [ ] [Understanding Deep Mind's Sorting Algorithm](https://justine.lol/sorting/)
- [ ] [The Secret Sauce behind 100K context window in LLMs: all tricks in one place](https://blog.gopenai.com/how-to-speed-up-llms-and-use-100k-context-window-all-tricks-in-one-place-ffd40577b4c?gi=7aa6ea601bd4)


### Projects
- [ ] [Karpathy's minGPT implementation](https://github.com/karpathy/minGPT)
- [ ] [PrivateGPT](https://github.com/imartinez/privateGPT)
- [ ] [QLoRA: Efficient Finetuning of Quantized LLMs](https://github.com/artidoro/qlora)
- [ ] [Guanaco 7B Colab example](https://colab.research.google.com/drive/17XEqL1JcmVWjHkT-WczdYkJlNINacwG7?usp=sharing)

### Papers
- [ ] [SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression](https://arxiv.org/pdf/2306.03078.pdf)